{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import csv\n",
    "import re\n",
    "import math\n",
    "from random import shuffle\n",
    "\n",
    "tweets = []\n",
    "nb_lines = 0\n",
    "\n",
    "# Patter for regular expression\n",
    "noUrl_patter = r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*'  # Remove links\n",
    "noTag_pattern = \"(?<!\\w)@\\w+\"  # Remove tags\n",
    "\n",
    "\n",
    "def preprocess_text(original_text):\n",
    "    text = original_text.lower()\n",
    "    noUrl_text = re.sub(noUrl_patter, '', text)\n",
    "    noTag_text = re.sub(noTag_pattern, '', noUrl_text)\n",
    "    noPunctuation_text = noTag_text.translate(None, string.punctuation)\n",
    "    noDigit_text = noPunctuation_text.translate(None, string.digits).strip()\n",
    "    list_words = noDigit_text.split(\" \")\n",
    "    if (\" \") in list_words:\n",
    "        list_words.remove(\" \")\n",
    "    return list_words\n",
    "\n",
    "dataset = open(\"Dataset.csv\", \"rb\")\n",
    "reader = csv.reader(dataset)\n",
    "reader.next()  # Skip the 1st line of the dataset which is the header\n",
    "for row in reader:\n",
    "    nb_lines += 1\n",
    "    text = row[3]\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    tweets.append([preprocessed_text, int(row[1])])\n",
    "\n",
    "shuffle(tweets)        \n",
    "        \n",
    "nb_learn = int(math.floor(nb_lines * 0.8))\n",
    "nb_train = int(math.floor((nb_lines-nb_learn)/2))\n",
    "nb_test = int(math.floor((nb_lines-nb_learn)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model uses 1262891 data to create the vocabulaire.\n",
      "This model uses 157861 data to train and 157861 to test\n"
     ]
    }
   ],
   "source": [
    "print(\"This model uses \" + str(nb_learn) + \" data to create the vocabulaire.\")\n",
    "print(\"This model uses \" + str(nb_train) + \" data to train and \" + str(nb_test) + \" to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_learn = [tweets[i][0] for i in range(nb_learn)]\n",
    "Y_learn = [tweets[i][1] for i in range(nb_learn)]\n",
    "\n",
    "all_words = []\n",
    "for data in X_learn:\n",
    "    all_words.extend(word for word in data)\n",
    "    \n",
    "all_diff_words = list(set(all_words))\n",
    "\n",
    "nb_words = len(all_diff_words)\n",
    "\n",
    "vocabulaire = {all_diff_words[i]: [i, 0] for i in range(nb_words)}\n",
    "\n",
    "def manipulate_vocabulaire(old_vocabulaire):\n",
    "    # Count the repitition of each word\n",
    "    for data in X_learn:\n",
    "        for word in data:\n",
    "            old_vocabulaire[word][1] += 1\n",
    "    \n",
    "    # List all the word which its repitition is smaller than 5\n",
    "    delete_keys = []\n",
    "    for key in vocabulaire.keys():\n",
    "        if old_vocabulaire[key][1] < 5:\n",
    "            delete_keys.append(key)\n",
    "            \n",
    "    # Delete the key in the vocabulaire\n",
    "    for key in delete_keys:\n",
    "        old_vocabulaire.pop(key)\n",
    "        \n",
    "    # Create the new vocabulaire\n",
    "    keys = old_vocabulaire.keys()\n",
    "    size = len(keys)\n",
    "    new_vocabulaire = {keys[i]: i for i in range(size)}\n",
    "    \n",
    "    return new_vocabulaire, size\n",
    "\n",
    "(vocabulaire,size) = manipulate_vocabulaire(vocabulaire)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulaire is 49881\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the vocabulaire is \" + str(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding\n",
    "from keras.layers import LSTM, SimpleRNN, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = [tweets[i + nb_learn][0] for i in range(nb_train)]\n",
    "Y_train = [tweets[i + nb_learn][1] for i in range(nb_train)]\n",
    "\n",
    "X_test = [tweets[i + nb_learn + nb_train][0] for i in range(nb_test)]\n",
    "Y_test = [tweets[i + nb_learn + nb_train][1] for i in range(nb_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'preparing', 'for', 'tests']\n",
      "['what', 'a', 'super', 'awesome', 'really', 'swell', 'day']\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[101])\n",
    "print(X_test[101])\n",
    "print(Y_train[101])\n",
    "print(Y_test[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "157861 train sequences\n",
      "157861 test sequences\n"
     ]
    }
   ],
   "source": [
    "max_features = size\n",
    "maxlen = 40  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 512\n",
    "\n",
    "print('Loading data...')\n",
    "for i in range(nb_train):\n",
    "    X_train[i] = [vocabulaire[j] for j in X_train[i] if vocabulaire.has_key(j)]\n",
    "    \n",
    "for i in range(nb_test):\n",
    "    X_test[i] = [vocabulaire[j] for j in X_test[i] if vocabulaire.has_key(j)]\n",
    "    \n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2528, 8245, 45156, 2853]\n",
      "[35774, 42638, 5746, 28406, 17864, 36413, 46989]\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_train[101])\n",
    "print(X_test[101])\n",
    "print(Y_train[101])\n",
    "print(Y_test[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "X_train shape: (157861, 40)\n",
      "X_test shape: (157861, 40)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen = maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen = maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 157861 samples, validate on 157861 samples\n",
      "Epoch 1/15\n",
      "157861/157861 [==============================] - 685s - loss: 0.5424 - acc: 0.7217 - val_loss: 0.4682 - val_acc: 0.7826\n",
      "Epoch 2/15\n",
      "157861/157861 [==============================] - 697s - loss: 0.4709 - acc: 0.7769 - val_loss: 0.4519 - val_acc: 0.7873\n",
      "Epoch 3/15\n",
      "157861/157861 [==============================] - 703s - loss: 0.4407 - acc: 0.7932 - val_loss: 0.4522 - val_acc: 0.7865\n",
      "Epoch 4/15\n",
      "157861/157861 [==============================] - 677s - loss: 0.4168 - acc: 0.8065 - val_loss: 0.4539 - val_acc: 0.7873\n",
      "Epoch 5/15\n",
      "157861/157861 [==============================] - 702s - loss: 0.3962 - acc: 0.8166 - val_loss: 0.4580 - val_acc: 0.7863\n",
      "Epoch 6/15\n",
      "157861/157861 [==============================] - 763s - loss: 0.3783 - acc: 0.8263 - val_loss: 0.4726 - val_acc: 0.7833\n",
      "Epoch 7/15\n",
      "157861/157861 [==============================] - 701s - loss: 0.3614 - acc: 0.8351 - val_loss: 0.4907 - val_acc: 0.7844\n",
      "Epoch 8/15\n",
      "157861/157861 [==============================] - 682s - loss: 0.3440 - acc: 0.8443 - val_loss: 0.5054 - val_acc: 0.7792\n",
      "Epoch 9/15\n",
      "157861/157861 [==============================] - 716s - loss: 0.3310 - acc: 0.8499 - val_loss: 0.5233 - val_acc: 0.7787\n",
      "Epoch 10/15\n",
      "157861/157861 [==============================] - 650s - loss: 0.3182 - acc: 0.8569 - val_loss: 0.5371 - val_acc: 0.7776\n",
      "Epoch 11/15\n",
      "157861/157861 [==============================] - 650s - loss: 0.3076 - acc: 0.8608 - val_loss: 0.5428 - val_acc: 0.7751\n",
      "Epoch 12/15\n",
      "157861/157861 [==============================] - 649s - loss: 0.2988 - acc: 0.8652 - val_loss: 0.5616 - val_acc: 0.7726\n",
      "Epoch 13/15\n",
      "157861/157861 [==============================] - 651s - loss: 0.2900 - acc: 0.8700 - val_loss: 0.5546 - val_acc: 0.7726\n",
      "Epoch 14/15\n",
      "157861/157861 [==============================] - 650s - loss: 0.2833 - acc: 0.8725 - val_loss: 0.5806 - val_acc: 0.7696\n",
      "Epoch 15/15\n",
      "157861/157861 [==============================] - 649s - loss: 0.2758 - acc: 0.8769 - val_loss: 0.6174 - val_acc: 0.7720\n",
      "157861/157861 [==============================] - 168s   \n",
      "Test score: 0.617429086997\n",
      "Test accuracy: 0.772001951112\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "h=model.fit(X_train, Y_train, batch_size = batch_size, nb_epoch = 15,\n",
    "          validation_data = (X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test, \n",
    "                            batch_size = batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': [0.72174254564366269, 0.7769240027543719, 0.79322315202178695, 0.80646264749523255, 0.81656013836806196, 0.82634089481407635, 0.83505108926375082, 0.84426805861494147, 0.84991859929443403, 0.85685508134337729, 0.86077625250216905, 0.86520419863270615, 0.86999322188130035, 0.87248908853022822, 0.87687269180074645], 'loss': [0.54238053578214507, 0.4708819278992678, 0.44065636519913437, 0.41676526371464645, 0.39617827669489381, 0.37827069717582479, 0.36138953994485379, 0.34402986505157679, 0.33102240165552932, 0.31817650576988971, 0.30764534471742749, 0.29878960991020392, 0.29003809382852352, 0.28330515286311175, 0.27579916351177308], 'val_acc': [0.78259988216538523, 0.78731922387869868, 0.78654639205947363, 0.78728121575763022, 0.78630567398957896, 0.78334104053415077, 0.78441793729775933, 0.7792108246992876, 0.77865970698797027, 0.77759547955991604, 0.77511861703986384, 0.7725910769767238, 0.77259741166960982, 0.76955042730332346, 0.77200195111224401], 'val_loss': [0.46819703915452932, 0.45191926172112185, 0.45217244934919715, 0.45387214593324859, 0.45797469224562865, 0.47260083360654659, 0.49073790253862876, 0.50535703840039214, 0.52326032938884937, 0.53710577773408685, 0.54277541920437089, 0.56159235266660001, 0.55459798204188315, 0.58057756200190658, 0.617429086997485]}\n"
     ]
    }
   ],
   "source": [
    "print(h.history)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
